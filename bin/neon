#!/usr/bin/env python
# ----------------------------------------------------------------------------
# Copyright 2014 Nervana Systems Inc.  All rights reserved.
# ----------------------------------------------------------------------------
"""
Driver script for running neon model experiments.
"""

import argparse
import datetime
import logging
import sys
from timeit import default_timer
import yaml

import neon
from neon.backends import gen_backend
from neon.util.persist import deserialize
from neon.util.trace import Tracer
from neon.util.metrics import dump_metrics


def parse_args():
    """
    Sets up and handles command line argument parsing.
    """
    parser = argparse.ArgumentParser(description='The Nervana Framework. '
                                     'This executable allows one to run '
                                     'experiments using a particular backend '
                                     '(defaults to single process [possibly '
                                     'multicore via BLAS multithreading] '
                                     'float32 CPU unless at least one of -g, '
                                     '-p, -m, -n, or -f are given).')
    parser.add_argument('yaml_file', type=argparse.FileType('r'),
                        help='experiment configuration settings')
    parser.add_argument('-v', '--version', action='version',
                        version=neon.__version__)
    parser.add_argument('-c', '--cloud', action='store_true',
                        help='Run your experiment in the Nervana Cloud')
    parser.add_argument('-g', '--gpu', action='store_true',
                        help='Attempt to run using a CUDA capable GPU backend')
    parser.add_argument('-n', '--nrv', action='store_true',
                        help='Attempt to run using the Nervana Engine hardware')
    parser.add_argument('-p', '--datapar', action='store_true',
                        help='Use parallelization to partition the data over '
                             'multiple nodes')
    parser.add_argument('-m', '--modelpar', action='store_true',
                        help='Use parallelization to partition the model over '
                             'multiple nodes')
    parser.add_argument('-f', '--flexpoint', action='store_true',
                        help='Attempt to run a Nervana FlexPoint(TM) capable '
                             'backend')
    parser.add_argument('-r', '--rng_seed', type=int,
                        help='Seed the random number generator for the backend'
                             ' with the specified value.')
    parser.add_argument('-i', '--device_id', type=int,
                        help='Select accelerator device id to run process on')
    parser.add_argument('-e', '--numerr_handling', action='store',
                        type=yaml.load, help='Set how numeric errors are '
                        'handled.  Python dict syntax, parameters are the same'
                        ' as for numpy.set_err')
    parser.add_argument('-s', '--profile', action='store',
                        help=('Collect and write profiling stats to the file '
                              'specified.  Or stdout if empty string.'))
    parser.add_argument('-o', '--output', action='store',
                        help=('Collect and write timing and a set of metrics '
                              'to the file specified.  Or stdout if empty '
                              'string.  We append if file exists'))
    parser.add_argument('-d', '--debug', action='store_true',
                        help='call ipython debugger on uncaught exception')
    parser.add_argument('-t', '--trace', action='store', nargs=2,
                        metavar=('file_filter', 'output_file'),
                        help='Trace through function calls. Optional '
                             'argument is a regex filter on the name of the '
                             'source files to confine the trace to.')
    return(parser.parse_args())


def main():
    """
    Point of code entry.
    """
    # setup an initial console logger (may be overridden in config)
    logging.basicConfig(level=30)  # WARN or higher
    # read in yaml configuration and initialize objects
    args = parse_args()
    if args.profile is not None:
        import cProfile
        p = cProfile.Profile()
        p.enable()
        logging.warn("Profiling code to: %s",
                     args.profile if args.profile != "" else "stdout")
    if args.trace is not None:
        tracer = Tracer.setup(args.trace[0], args.trace[1])
    if args.output is not None:
        start_date = str(datetime.datetime.now())
        start_time = default_timer()
        logging.info("Outputting timing and metrics to: %s",
                     args.output if args.output != "" else "stdout")
    if args.cloud:
        # TODO: kick-off cloud job
        logging.warning('Sorry!  We still need to add hooks for the Nervana '
                        'cloud. In the meantime your Experiment will be run '
                        'locally.')
    try:
        experiment = deserialize(args.yaml_file)
        if hasattr(experiment, 'logging'):
            logger = logging.getLogger()
            handler = logger.handlers[0]
            if "filename" in experiment.logging:
                logging.warn("replacing console logging with file logging: %s",
                             experiment.logging["filename"])
                logger.removeHandler(handler)
                handler = logging.FileHandler(experiment.logging["filename"])
                logger.addHandler(handler)
            if "format" in experiment.logging:
                formatter = logging.Formatter(experiment.logging["format"])
                handler.setFormatter(formatter)
            if "level" in experiment.logging:
                logging.warn("setting log level to: %d",
                             experiment.logging["level"])
                logger.setLevel(experiment.logging["level"])
        if args.output is not None:
            # compute a standard set of metrics
            experiment.inference_sets = ["train", "test", "validation"]
            experiment.inference_metrics = ["misclass rate", "auc", "log loss"]
        # carry out the experiment
        if not hasattr(experiment, 'backend') or experiment.backend is None:
            backend = gen_backend(model=experiment.model, gpu=args.gpu,
                                  nrv=args.nrv, datapar=args.datapar,
                                  modelpar=args.modelpar,
                                  flexpoint=args.flexpoint,
                                  rng_seed=args.rng_seed,
                                  numerr_handling=args.numerr_handling,
                                  device_id=args.device_id)
        else:
            backend = experiment.backend
            logger.info("utilizing %s backend from yaml file",
                        backend.__class__.__name__)
        experiment.initialize(backend)
        result = experiment.run()
        if args.profile is not None:
            if args.profile == "":
                p.print_stats("tottime")
            else:
                p.dump_stats(args.profile)
        if args.output is not None:
            # fill in any missing metric values
            if result is None:
                result = dict()
            for metric in experiment.inference_metrics:
                if metric not in result:
                    result[metric] = dict()
                for dset in experiment.inference_sets:
                    if dset not in result[metric]:
                        result[metric][dset] = float("nan")
            dump_metrics(args.output, args.yaml_file.name,
                         start_date, default_timer() - start_time, result)
        return experiment, result, 0
    except Exception as e:
        if args.debug:
            import ipdb, traceback
            traceback.print_exc()
            ipdb.post_mortem(sys.exc_info()[2])
        else:
            raise

if __name__ == '__main__':
    experiment, result, status = main()
    sys.exit(status)
