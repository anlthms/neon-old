Nervana internal library to implement deep neural networks (in floating point).


DBN/
    test_mnist.py: Code to train DBN using CD-1
    test_mnist_nn.py: Code to train DNN using backprop (optionally using a pretrained DBN)
    
[todo: refactor these at some point]    
    rbm_cudamatclass.py: RBM cudamat class
        rbm_cudaclass.py: RBM class using gnumpy (slower)
        rbm_class.py: RBM numpy class (slow)
    dbn_class.py: DBN class
    nn_class.py: NN class (uses cudamat)
    
[todo: add DAE, CNN classes]

Best parameters so far for MNIST:

DBN: 
2.36-2.76% [on real test set, 10k/60k, vs 10k/50k before; eta=.009, wtinit=.01, mom: 0.99[or 0.9] ->0.9, free energy, penalty=.0002, using sigmoid]

NN:
1.21% [2000x1000x1000 network with dropout, wt init to N(0,.01), using ReLu, no pre-training, eta=.1, mom=0.9->0.9, penalty=.000001(or higher)]