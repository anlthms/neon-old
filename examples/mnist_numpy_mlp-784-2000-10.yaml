# Experiment settings to train/test a 1 hidden layer Neural Net on full
# MNIST.  Parameters are optimized for performance.

backend: {
  type: mylearn.backends._numpy.Numpy,
  rng_seed: 0,
}

datasets: [
  # MNIST data
  {
    #type: !!python/object:dataset.MNIST,
    type: mylearn.datasets.mnist.MNIST,
    repo_path: '/usr/local/data',
    sample_pct: 100,
    serialized_path: '/usr/local/data/MNIST/mnist-Numpy.pkl',
  },
]

# simple MLP model specification
model: {
  type: mylearn.models.mlp.MLP,
  layers: [
    {
      name: h0,
      connectivity: full,
      num_nodes: 2000,
      activation: mylearn.transforms.rectified.RectLin,
      weight_init: {
        type: normal,
        loc: 0.0,
        scale: 0.01,
      },
    },
    {
      name: output,
      connectivity: full,
      num_nodes: 10,
      activation: mylearn.transforms.logistic.Logistic,
      weight_init: {
        type: node_normalized,
        scale: 4.0,
        bias_init: 0.0,
      },
    },
  ],
  num_epochs: 60,
  batch_size: 100,
  learning_rate: 0.01,
  momentum: {
    type: linear_monotone, #constant, nesterov
    initial_coef: 0.99,
    start_epoch: 5,
    saturate_epoch: 5,
    saturated_coef: 0.90,
  },
  cost: mylearn.transforms.cross_entropy.CrossEntropy,
  serialized_path: './mlp-simple.pkl',
}

# dumping of predictions/output?

# logging options that are passed to logging.basicConfig
# level value thresholds (set level lower to display them):
#   CRITICAL 50
#   ERROR    40
#   WARNING  30
#   INFO     20
#   DEBUG    10
#   NOTSET    0
logging: {
  level: 20,
  format: '%(asctime)-15s %(levelname)s:%(module)s - %(message)s'
}
