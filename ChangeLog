# Changelog

## (unreleased)


## v0.8.0 (2015-04-20)

### Modifications

* v0.8.0 release bump. [Scott Leishman]

* Updated architecture diagram, minor doc changes. [Scott Leishman]

* Ensure batch writer calculates correct number of classes. [Scott Leishman]

* style fix. [Scott Leishman]

* Merge branch 'MYL-235_decode_macrobatch_in_bg_thread-3' into 'master' [Urs Koster]

    Myl 235 decode macrobatch in bg thread 3  Move macro batch loading and
    decoding as well as mini batch image/label transposing and data type
    conversion to a background thread using double buffering. Mini batch
    processing time is improved by about 10% as a result.  See merge
    request !124

* style fix. [Urs Koster]

* reapplied batch normalization fix and documented what is going on. [Urs Koster]

* newline. [Urs Koster]

* newline. [Urs Koster]

* hotfix for onehot fix. [Urs Koster]

* Merge branch 'fp16fixes' into 'master' [Scott Leishman]

    Fp16fixes  Metrics handling for imagenet / imgworker models.  See
    merge request !122

* Merge branch 'fp16fixes' into 'master' [Scott Leishman]

    Fp16fixes  Followup on the merge request for the new fp16/fp32 GPU
    backend: Improved handling of default dtypes.  See merge request !118

* Merge branch 'hyperopt2' into 'master' [Scott Leishman]

    Hyperopt2  Improvements to hyperparameter optimizations:  - experiment
    directory is no longer hardcoded but passed as HYPEROPT_PATH variable
    - spearmint is no longer assumed to be in a known location but passed
    as SPEARMINT_PATH variable  - instead of a custom experiment type to
    write the error to a file, now uses validation error returned through
    metrics functionality.  See merge request !121

* Merge branch 'MetricsOnlineReporting' into 'master' [Urs Koster]

    Metrics online reporting  Handful of small updates to allow multiple
    metrics of the same type for each dataset.  See merge request !120

* Merge branch 'metrics' into 'master' [Alex Park]

    Major metrics refactor, performance comparison utility  Metrics are
    now specified as classes, and calculated entirely on host.  New `neon
    -o` output flag allows one to append timing and performance
    measurements to a file for subsequent comparison.  New
    `compare_metrics` executable can display latest performance of a given
    example against prior runs (to help identify performance or timing
    degradations).  Closed several lingering issues around imgworker
    importing, and extending neon.  Lots of documentation updates and
    enhancements.  See merge request !119

* Strange relu hack on autouniform val gen.  Discovered by Urs and seems to make everything better -- too good not to include. [Alex Park]

* Merge branch 'sgraykernels' into 'master' [Scott Leishman]

    MAX16 backend and instrumentation  Three major features in this
    branch: * new GPU backend, which is a lightweight wrapper for the
    nervanagpu libraries (Maxwell architecture,   fp16/fp32 support) *
    instrumentation for timing plots and monitoring raw activation values
    using decorators * batch normalization (via BatchNormAlex branch)  See
    merge request !116

* Merge branch 'ImageSetNew' into 'master' [Scott Leishman]

    Image set new  Includes imgworker and boost as dependencies and
    creates new ImageSet dataset type. Also adds some metric code for
    calculating logloss, top1, topk in one go.  See merge request !117

* Merge branch 'GabrielPereyra-leaky_rect' [Scott Leishman]

    Added and augmented from Gabriel's request.

* Merge branch 'augdata' into 'master' [Scott Leishman]

    Augdata  Bug fixes, enhancements and cleanup  Deprecated code that is
    no longer used. Added a new parameter to dataset.get_mini_batch() to
    request for data augmentation. Added seeding for getting consistent
    results with serialized models.  See merge request !115

* Merge branch 'cleanup2' into 'master' [Scott Leishman]

    Cleanup2  Move hurricane dataset out into its own repo.  See merge
    request !114

* Merge branch 'ConvSharedBiases' into 'master' [Anil Thomas]

    Conv shared biases  Implements the use of shared biases for
    convolutional layers (and makes it the default mode for biases) Shared
    biases means that there is a single bias for each output feature map
    rather than for each activation value. so for an output that is (nofm
    * ofmshape[0] * ofmshape[1], batch_size), there will be (nofm, 1)
    biases.  The code makes liberal use of tensor reshaping so that
    accumulation can be done across featuremap and batch dimensions.  See
    merge request !113

* Merge branch 'GlobalDeltaBuffer' into 'master' [Anil Thomas]

    Global delta buffer  Modifies MLP to be able to allocate a single
    global delta buffer pool for all layers (except data and cost) to
    conserve memory  See merge request !112

* Bugfix / typo to hyperopt example yaml. [Urs Koster]

* Bugfix to the '`pwd`' blunder, additional info about dependencies, fixed yaml file path in hyperopt example. [Urs Koster]

* fix to RNN bug. [Urs]

* Merge branch 'cleanup' into 'master' [Scott Leishman]

    Cleanup  Moved NDSB related code to a different repository.  See merge
    request !111

* minor documentation and example updates. [Scott Leishman]

* Merge branch 'documentation' into 'master' [Scott Leishman]

    Documentation  added top level documentation for layers, learning
    rules.  See merge request !110

* Merge branch 'metrics' into 'master' [Scott Leishman]

    Metrics  Selective merge from the BCI branch. This is to support
    saving the result to a file and computing AUC.  Use the tag
    &quot;inference_sets&quot; at the top level to specify the sets whose inference
    values are to be saved. Use the tag &quot;inference_metrics&quot; to specify the
    error metrics to be computed on those sets.  e.g.
    !obj:experiments.FitPredictErrorExperiment {   inference_sets:
    ['test'],   inference_metrics: ['log loss', 'misclass rate'],   ...
    See merge request !109

* Merge branch 'specify_gpu' into 'master' [Arjun Bansal]

    Specify gpu  Support for specifying gpu id when creating gpu backend
    Support for using multiple GPUs when using par backends  See merge
    request !108

* Minor neon help description update. [Scott Leishman]


## v0.7.1 (2015-02-14)

### Modifications

* v0.7.1 release bump. [Scott Leishman]

* Merge branch 'misc_fixes' into 'master' [Arjun Bansal]

    Miscellaneous fixes  Corrects various issues introduced in 0.7.0
    including: * automatic import of certain optional dependencies where
    not required * rough-in support for Nervana hardware backend command
    line parameter * python3 compatibility fixes * removal of extra -I
    argument to extension compilation where numpy not installed * support
    non-path filenames for serialization and deserialization * cudanet
    version support (skip re-install if version already installed) *
    remove source code links from published documentation * fix
    deserialization of models and layers, re-support checkpointing via
    overwrite_list parameter.  See merge request !106

* Merge branch 'vgg' into 'master' [Alex Park]

    MYL-174 Implemented VGG/ reduced memory footprint  Implemented a
    barebones VGG model.  RectLin function in gpu.py was modified to use a
    new function in cudaconvnet2 backend called maximum_scalar (Returns
    maximum of a tensor and a scalar), so it wouldnt need an extra buffer.
    For code cleanliness I eliminated the need for 'activation is not
    None' by defining a default Linear activation function, assigned in
    layer.py (have made these changes for pooling.py, recurrent.py, and
    fully_connected.py also). @urs can you please confirm that the
    recurrent changes are ok?  I have changed activations to just have a
    activation.fprop_func() and activation.bprop_func(), so we can decide
    what to compute during fprop and bprop for each activation in the most
    computationally and memory efficient way. I got rid of apply_both()
    (mostly replaced with fprop_func instead).  There is still overhead
    despite these changes. We need a way to consolidate the delta buffers
    across layers by allocating a large pool and then sub-indexing the
    chunk used for each layer. Not sure how easy this will be on the gpu
    or if there is some other smarter way of doing this.  Ran make test
    and sanity, and some MLP and convnet MNIST examples which all seemed
    to run fine.  Ran autopep8 and make style  See merge request !105


## v0.7.0 (2015-02-09)

### Modifications

* v0.7.0 release bump. [Scott Leishman]

* additional documentation cleanup. [Scott Leishman]

* Merge branch 'refactor_updates' into 'master' [Anil Thomas]

    Various refactor updates  * removed epsilon from backend * added new
    utility script to inject/strip compiler hints (begin/end/free calls) *
    new shortcut import notation for import models, layers, datasets, etc.
    * re-organized layer code into separate directory and files. * added
    support for environment variable and home directory parsing in YAML
    files * re-factored backend loading and initialization.  Now specified
    on command-line and no longer needed in YAML files. * re-organized
    example directories, deprecated several redundant examples. *
    documentation audit and updates.  See merge request !104

* Merge branch 'par' into 'master' [Scott Leishman]

    Par  Add new command line parameters --datapar and --modelpar.  See
    merge request !103

* Merge branch 'rnn' into 'master' [Scott Leishman]

    Rnn  RNN and LSTM have been updated to the new layer2 format.
    Following pieces are now deprecated and can be moved:  - the RNN class
    in rnn.py, (the RNNB class can be renamed to RNN and the yamls
    updated)  - the Recurrent layer classes in layer.py can be deprecated
    with the rest of the file  - the yaml files
    `mobydick_cpu_lstm-128-64-5.yaml` and `mobydick_cpu_rnn-128-64-5.yaml`
    can be deleted or moved.  An additional change not related to layer2
    is that the RNN now supports full backprop through time in addition to
    the standard truncated BPTT. The prediction code has also been greatly
    simplified, following the layer2 MLP example.  See merge request !102

* Merge branch 'par' into 'master' [Alex Park]

    Par  See merge request !101

* Minor bug fix in step_print in mlp.py. [Arjun Bansal]

* Merge branch 'systems' into 'master' [Scott Leishman]

    Systems  - modified nrv backend yaml files to have logging and flex
    support - propagated the broadcast work-around to cross_entropy.py
    See merge request !99

* test error down to 1.88% [Arjun Bansal]

* matching learning rate to mlp1.py ref example. [Arjun Bansal]

* Merge branch 'AccumFix' into 'master' [Scott Leishman]

    Accum fix  See merge request !97

* Merge branch 'hyperopt_reverted' into 'master' [Scott Leishman]

    Hyperopt reverted  new merge request! Passes sanity and test, also
    passes  PYTHONPATH='.' bin/hyperopt init PYTHONPATH='.' bin/hyperopt
    run PYTHONPATH='.' bin/hyperopt reset  and produces output at
    http://localhost:5000 on my machine.  See merge request !96

* Merge branch '3dconv' into 'master' [Arjun Bansal]

    3dconv  See merge request !95

* Merge branch 'systems' into 'master' [Scott Leishman]

    Systems  fix for the broadcast workaround in mlp.py  See merge request
    !93

* Merge branch 'mop_updates' into 'master' [Arjun Bansal]

    deserialization and weight initialization improvements  *
    Deserialization has been re-written to work with various distributed
    and non-  distributed datasets. * new deserialization_path parameter
    to support separate files to read from vs write to during
    serialization * weight initialization code has been refactored and
    split out into separate classes (removed backend specific code
    duplication). * new shortcut parameter initialization approach allows
    yaml specifications like `!params.UniformValGen` to work correctly. *
    epochs_complete parameter allows restoration of partially complete
    training during serialization/deserialization * ability to overwrite
    specified deserialized attributes with yaml values based on list in
    overwrite_list * all examples updated to reflect new weight
    initialization scheme * various style and bug fixes.  See merge
    request !92

* Merge branch '3dconv' into 'master' [Alex Park]

    3dconv  See merge request !91

* Merge branch 'DatasetFix' into 'master' [Scott Leishman]

    Dataset fix  Changed default behaviour for the experiment to determine
    the dataset for both training and prediction.  Updated yaml files to
    remove the dataset from the DataLayer and have it set when calling
    model.fit or model.predict_and_error  See merge request !90

* Merge branch 'SanityFix' into 'master' [Scott Leishman]

    Sanity fix  More changes than I had anticipated.  Some style changes
    (mostly spacing) corrections to the dataset interface in mlp.MLP and
    to the way the experiment results are unwrapped by the sanity checker.
    Not sure if this the way we want to go about this.  See merge request
    !89

* Merge branch 'distarray' into 'master' [Alex Park]

    ImageNet support (with multithreading)  Merged in ImageNet changes
    with queues, with new Layer2.py style implementation. Added bias for
    ConvLayer, learning rate schedule, Weight decay, separate learning
    rules for biases &amp; weights to new layer2.py style implementation.
    Perf: Around 7.5s/macrobatch for AlexNet on 980 and ~64% training and
    88% validation error (top-1) after 20 epochs on micro set (25 train
    epochs, 5 val epochs). Takes around an hour for a run of
    i1k_gpu_small.yaml.
    todo: dist support
    todo: multiprocessing decompression of jpegs  See merge request !85

* Reverting prior two rnn commits. [Scott Leishman]

* RNN created a yaml file with a standard RNN (not LSTM) layer. [Urs]

* RNN bugfix to logger info missing asnumpyarray. [Urs]


## v0.6.0 (2015-01-14)

### Modifications

* v0.6.0 release bump. [Scott Leishman]

* Style fixes. [Scott Leishman]

* Merge branch 'rnn' into 'master' [Alex Park]

    Rnn  The main thing here is the clip in neon/transforms/tanh.py
    everything else is sugar.  See merge request !88

* Small changes to better use parent class initialization in Balance Model. [Alex Park]

* Merge branch 'mop_updates' into 'master' [Alex Park]

    MOP API conformance updates  Large number of changes to backend code
    to bring into conformance with MOP API and ensure consistency between
    current CPU and GPU implementations.  Includes: * remove wrap usage *
    make raw a read-only attribute * allow numeric scalar input parameters
    for binary comparison and manipulation ops * replace clear with fill
    and publish * add GEMM functionality to dot * standardize dtype usage
    * ensure CPUTensor's have min 2 dimensions, vectors default to Nx1
    column vectors * ensure all reduction ops return Tensor's in all
    implementations * update unit tests, various documentation updates *
    update all of above to work with new layer structure  See merge
    request !87

* Patch get_berrbuf bug introduced in previous commit. [Scott Leishman]

* Merge branch 'rnn' into 'master' [Alex Park]

    Rnn  LSTM layer has been fleshed out and fully integrated into the RNN
    code. It passes gradient checks but crashes with an overflow error
    after a while so it's not quite ready. Creating a merge request mainly
    to resolve questions about how the cost function shortcut.  See merge
    request !86

* Merge branch 'ndsb' into 'master' [Alex Park]

    Ndsb  See merge request !84

* Merge branch 'systems' into 'master' [Scott Leishman]

    Systems: Adding yaml config for IRIS/NRV backend  The IRIS CPU/config
    file keeps changing. Putting the NRV backend config file here may help
    developers to modify the NRV backend yaml file when they modify the
    CPU version.  See merge request !83

* Merge branch 'LayerRewrite' into 'master' [Scott Leishman]

    Layer rewrite  Creates a new set of layers that do not allocate upon
    creation.  Better hierarchy for layers overall.  - Simplified model
    code  - Added data layers and cost layers.  - Added crossmap pooling
    for gpu backend via 1x1 convolution  - Added Krizhevsky style local
    contrast normalization as backend function  - Added local option for
    convolution, (for local filtering with unshared weights).  cpu version
    still seems to have bug in bprop -- working to resolve.  - yaml files
    are simplified by virtue of not having to define learning rules in-
    line, inferring input feature map size, and eliminating need to
    position variable by just being aware of whether previous layer is a
    data layer or not  - Folded accumulation into weight layers rather
    than having separate multipass versions.  See merge request !82

* Added non-commercial license details. [Scott Leishman]

* Merge branch 'ndsb' into 'master' [Alex Park]

    Ndsb  See merge request !80

* Merge branch 'mop_updates' into 'master' [Anil Thomas]

    Remove symbolic arithmetic operators  * Replace all symbolic
    arithmetic operator backend calls (+, -, *, /, **) with our named
    equivalents.  See slight performance improvement in CNN networks, no
    real change in other networks.  Updated all existing model and layer
    code to reflect this. * Update published documentation location and
    references.  See merge request !81

* Undoing the bad commit. [Alex Park]

* something like mlp1 reference yaml. [Alex Park]

* Merge branch 'perf2' into 'master' [Scott Leishman]

    Perf2  Load the dataset as a list of minibatches.  See merge request
    !79

* Merge branch 'balance' [Alex Park]

* style fix. [Scott Leishman]

* Merge branch 'balance' into 'master' [Alex Park]

    Balance  See merge request !78

* Merge branch 'balance' into 'master' [Scott Leishman]

    Balance  * Addition of autoscale weight initialization and
    orthogonalized random weights initialization a la Andrew Saxe in
    http://arxiv.org/abs/1312.6120 * Addition of unpooling function for
    GPU backend * Addition of a generate output experiment for visualizing
    balance network output * Addition of MultiPass layer for being able to
    accumulate parameter update gradients * Addition of layer name
    linkages from yaml file to create layer graph * Addition of scaling
    parameter to cost functions * Refactoring of ada_delta into cpu and
    gpu backends * Fix for xcovariance cost function * Fix for avg pool
    and l2 pool functions in GPU backend * Fix assignment of scalar to
    GPUTensor slices * Fix for softmax and logistic shortcut derivative
    when receiving back propagated error from non cross entropy cost
    function  See merge request !77

* Merge branch 'mop_updates' into 'master' [Anil Thomas]

    Higher Level Function Consistency  * Published fully connected,
    convolutional, pooling, and crossmap response norm
    forward/backward/update functions to our MOP layer * re-paramaterized
    each of the above to enforce consistency, added docstrings for all. *
    re-factored pooling code to reduce code duplication.  Now requires op
    parameter specifying the type of pooling to carry out.  See merge
    request !76

* Merge branch 'rnn' into 'master' [Scott Leishman]

    Rnn  This is a minor merge request that should not be ** included into
    the version log.  Not quite sure if this is ready to go into master,
    it incorporates both perf2 and mop_updates that may not even be in
    master yet. I just want to get it out because it will be unproductive
    for others to make the pre-refactor RNN code work with any new
    changes. The diff against the current master looks heinously
    complicated, but this should be much simpler once perf and mop are in.
    It would be good if this could be tested on the GPU, which was the
    main purpose of refactoring. I still don't have cudnet running.  See
    merge request !75

* Merge branch 'perf2' into 'master' [Alex Park]

    Perf2  See merge request !74

* Merge branch 'perf2' into 'master' [Alex Park]

    Perf2  LayerWithNoBias will no longer exist after this merge. Going
    forward, use the &quot;Layer&quot; class instead. Omit the bias_init parameter
    in the yaml file to indicate that biases should not be used. Likewise,
    use LayerDist instead of LayerWithNoBiasDist.  The backend API
    &quot;append_bias()&quot; also goes away. The biases are kept in a separate
    vector and learned separately.  See merge request !73

* Merge branch 'systems' into 'master' [Urs Koster]

    Systems  It is useful for me to see which backend function calls are
    being called and what the nesting of the calls is. So I added a
    trace.py to neon.util which will help trace the function calls. After
    chatting with Urs yesterday I created a new neon branch called systems
    and checked in the trace facility to the systems branch (see commit
    eabf7c067 ). Possible to integrate this to the neon/master branch?
    Since tracing is optional (defined from command line with a -t option
    like with the -p or -d options), this has no runtime penalty on the
    core neon functionality.  Here is how you use trace:  bin/neon --trace
    &quot;(mlp.*\.py)|(cpu.py)|(layer.py)&quot; abc.trace
    examples/iris_cpu_mlp-4-2-3.yaml  - The first argument to --trace is a
    regex file filter. Tracing is only done on filenames which match this
    regex. This is useful, for example, to see how and where layer.py
    calls cpu.py. - The second argument is the name of the file to log to.
    stdout is not affected.  I would like to have my development setup
    always pass a --trace argument so that I can quickly look back into
    the call order if needed.  Happy to accommodate code reviews, of
    course.  Thanks, Aravind  See merge request !72

* Merge branch 'mop_updates' into 'master' [Anil Thomas]

    lazy log evaluation, efficient range in python 2 and 3  Ensure all
    logging parameters are lazily evaluated (only computed if that level
    of logging is enabled). Create a python 2 and 3 compliant range
    iterator and use that in place of built-in xrange/range. Clarify and
    update some items in the annotated example, make it easy to run all
    example networks consecutively.  See merge request !71

* Merge branch 'gpu_slice_fix' into 'master' [Anil Thomas]

    gpu 1D slice fix  Fix to support 1D slice notation during assignment
    in the GPU backend.  Inputs of the form `x[key] = value` are
    interpreted and treated the same as `x[key, :] = value`  See merge
    request !70

* Merge branch 'perf2' into 'master' [Alex Park]

    Perf2  This is faster at the expense of memory. The speed increase is
    only noticeable if the network is mostly LRN layers.  See merge
    request !69

* v0.5.0 updated Changelog. [Scott Leishman]


## v0.5.0 (2014-12-04)

### Modifications

* v0.5.0 release bump. [Scott Leishman]

* Merge branch 'balance' into 'master' [Anil Thomas]

    Balance  many changes...  sorry to tag you anil, but i didn't want to
    wear scott out before sending him into below freezing weather  -
    update() function refactored out of bprop()  - costs changed to refer
    to arbitrary layer along with output buffer (so now you just supply
    target to the cost computation)   - above allows us to remove temp
    buffer allocations separately from the cost instantiation   - see
    .yaml files to see new convention for cost instantiation   - necessary
    for balance layer where there will be multiple costs for a network,
    each connected to different layers   - can also change reference to
    which buffer to is used if a layer has multiple outputs (see rnn for
    how to use cost.set_outputbuf)   - set_outputbuf also used a lot in
    the gb.py model  - addition of BranchLayer, allows splitting of layers
    at the same level  - addition of softmax, xcov, hinge_l2 cost
    functions  - streamlining of activation functions  See merge request
    !68

* Merge branch 'distarray' into 'master' [Scott Leishman]

    ImageNet support (no multithreading)  MYL-88 MYL-89 MYL-119 Tested on
    the new i1k_gpu_alexnet.yaml file and cifar10_gpu_cnn.yaml file  See
    merge request !67

* Merge branch 'perf2' into 'master' [Scott Leishman]

    Perf2  Support fully connected layers with no activation.  See merge
    request !66

* forgot a comma, sorry. [Urs]

* fixed two bugs introduced by the attempted bugfix to linear_monotone momentum: momentum was actually constant at 0.9 in all the examples, the .99 was never reached. This affeced all yaml files. The second fix is to check_grad.py, where the numerical gradient is rescaled by the batch size and output layer size to reflect the change in cross_entropy from mean() to sum() [Urs]

* Merge branch 'rnn' into 'master' [Scott Leishman]

    RNN  This is a pretty big chunk of new code with the RNN model, it's
    layers, some visualization code and the Moby Dick data set. Changes to
    learning rule, activation etc. are fairly minimal, but might have some
    (as of yet undiscovered) effects on current and future models. Make
    sanity passes, but for example tahn has been changed to follow the new
    format of sigmoid, so any model that uses it would need to be updated.
    Similarity, there were some strange things going on with
    linear_monotone momentum, which I think I fixed, but that might lead
    to strange results if the old behavior is expected. I am not yet 100%
    sure this network is bug free, as there appear to be some subtle
    differences to the reference_implementations/rnn2.py which should do
    the same thing (and has been checked by numerical gradient). Since I
    am shifting my focus to LSTMs now, I want to get it into master in
    this state anyway.  See merge request !65

* Merge branch 'ResponseLayerFeature' into 'master' [Scott Leishman]

    Response layer feature  This branch adds a cross-map response
    normalization layer and some missing pooling operations.  - average
    pooling and l2 pooling for gpu backend (requires latest cudanet) -
    CrossMap response norm along with gpu and cpu fprop and bprop
    functions (MYL-113) - also added cpu backend power() function (we
    should be careful to handle negative numbers properly because
    numpy.power can have weird behavior)  See merge request !64

* Merge branch 'build_cleanup' into 'master' [Anil Thomas]

    Build cleanup - restrict default requirements and fix build
    environment issues  Tired of build failed emails?  Well this set of
    commits should address this as well as more appropriately handle
    requirements installation (when coupled with the updated cmake based
    install of cuda-convnet-2, see the [merge
    request](http://gitlab.localdomain/algorithms/cuda-
    convnet2/merge_requests/6) -- for the moment I've disabled GPU based
    tests in tox on the CI servers while this is being reviewed).  Now
    you'll need to explicitly edit `setup.cfg` once, or pass in parameters
    to your `make` calls to get GPU or distributed functionality, which is
    documented [here](http://atlas.localdomain:5700/using_neon.html
    #configuration-setup).  I can only assign this commit to 1 individual
    (sorry @anil, you're the unlucky recipient!) but @alex, @arjun, @urs,
    @amir it would be great if you could each check out this branch and
    try and install/test on whatever machine you use to develop to make
    sure there aren't any lingering issues, or if the documentation isn't
    clear.  Please report any feedback in the comments and I'll try and
    address them.  See merge request !63

* Conditionally import GPU only when CUDA card is installed. [Scott Leishman]

* Merge branch 'perf2' into 'master' [Scott Leishman]

    Perf2  See merge request !62

* Fix bug with dropout layer integration. [Anil Thomas]

* Merge branch 'DropoutFeature' into 'master' [Anil Thomas]

    Dropout feature  This is just the dropout stuff but added on top of
    the most recent master (as opposed to the one that was branched off of
    GPU_MPI a week ago).  Passes make test and make style  See merge
    request !61

* Merge branch 'mop_updates' into 'master' [Scott Leishman]

    Copyright notice updates  Touches nearly every file to add our
    copyright notice.  Also small update to ensure that when we run
    `publish_docs` permissions are not set too restrictive.  See merge
    request !60

* Merge branch 'GPU_mpi' into 'master' [Scott Leishman]

    Gpu mpi  Changes in this branch: - Correct behavior of col/row slice
    as far as views and copies go - Makes use of new cudanet capability
    for device selection - Implements GPUDist backend for data parallel -
    Fixes some potentially very bad behavior of dtype overriding (passing
    None for dtype overrides default value of np.float32) - Corrects
    problem of streaming for argmax and argmin kernels where data was
    being read prior to kernel execution completion - Needs latest master
    of cudanet in order to work  See merge request !59

* Merge branch 'mop_updates' into 'master' [Scott Leishman]

    argmin, argmax, p-norm, other cleanup and unit tests  Finished
    revamping argmin, argmax, norm across each of the three backends.
    Wrote unit tests for each, updated docs and so forth.  See merge
    request !58

* revert previous gpu unit test change. [Scott Leishman]

* temporary work-around to get builds running ok. [Scott Leishman]

* fix broken GPU tensor unit test. [Scott Leishman]

* Merge branch 'perf2' into 'master' [Scott Leishman]

    Perf2  See merge request !57

* Temporarily remove optional package requirements. [Scott Leishman]

    This will break tests for the moment!

* Restore virtual environment tox path. [Scott Leishman]

* Use cpu not numpy backend for cnn adadelta. [Scott Leishman]

* Merge branch 'master' of https://github.com/NervanaSystems/neon. [Scott Leishman]

    Merging in Amir's license fix from github.  Closes MYL-116

* Included square and cube into cudanet backend. [Alex Park]

* Adding an adadelta cnn for numpy. [Alex Park]

* Merge branch 'mop_updates' into 'master' [Scott Leishman]

    Mop updates - remove logical operators, add unit tests and doc
    strings, new functions.  Partially closes MYL-85  Several other
    additions and cleanups too.  See merge request !55

* Merge branch 'distarray' into 'master' [Scott Leishman]

    Implements data parallel MLP and convnet.  Earlier models were using
    the halo/tower strategy or vector parallel strategy. For the GPU
    cluster, the simplest thing we want to try are data parallel models.
    This implements an MLP and convnet (for CPU backend: step 1 of
    MYL-92). Step 2 will be GPU backend support.  Also updated the MPI
    docs (though WIP).  *Master was merged in before this request  See
    merge request !54

* Remove un-needed modules. [Scott Leishman]

* Merge branch 'perf2' into 'master' [Scott Leishman]

    Perf2  Sanity tests.  See merge request !51

* Merge branch 'distarray' into 'master' [Scott Leishman]

    distributed MLP support for layers with bias  merged in master before
    submitting merge request  See merge request !53

* Merge branch 'mop_updates' into 'master' [Scott Leishman]

    CI build fixes, consistent transpose  * Ensure Linux based builds can
    find CUDA libs during tox run.  Needed for our new CI server * fix
    small python3 incompatibility. * get rid of .T in existing code, force
    use of .transpose() in backends, models, and layers.  See merge
    request !52

* Cleanup flexpoint make clean target. [Scott Leishman]

* Merge branch 'mop_updates' into 'master' [Scott Leishman]

    Mop updates - round 1  Completes first pass of MOP updates.  Biggest
    changes include: * force instance creation for each backend operation
    (cleans up inconsistent mix of class/static methods) * backends and
    tensors have been renamed, now have CPU (numpy derived) and GPU (cuda-
    convenet2 derived) backends, with existing _numpy, _cudamat, _cudanet
    backends and tensors relegated to an unsupported subdir.   * We may
    drop unsupported backends altogether in the future, but for now we
    keep them around.  We will not push functionality updates to them
    though   * required renaming all example networks, unit tests, and so
    forth * renamed fixed decimal point work to flexpoint, applied a
    couple of fixes to match updates to other backends and layer/model
    changes.   * This work is still incomplete so flexpoint examples
    remain broken * audited documentation so that it reflects current set
    of models, layers, datasets implemented.  See merge request !50

* Merge branch 'distarray' into 'master' [Scott Leishman]

    Fix dist and non-dist inconsistencies following recent merges  GB and
    GB dist match (doesn't fix MYL-102 though) Convnet and Convnet Dist
    match  With MLP there is small divergence between dist, non-dist, and
    GPU non-dist. Not sure what is causing it or whether it is
    significant.  See merge request !49

* Minor license updates. [Scott Leishman]

* Merge branch 'LearningRuleRefactor' into 'master' [Scott Leishman]

    Learning rule refactor  See merge request !47

* Merge branch 'license' into 'master' [Scott Leishman]

    License  See merge request !48

* Merge branch 'cudaconvnet_integration' [Scott Leishman]

    Manually resolved some dataset related conflicts.  Seeing some issue
    with GB distributed net and possibly others.  Checking into mainline
    so that Arjun can help debug.

* Merge branch 'distarray' into 'master' [Scott Leishman]

    simplify install and add initial dist MLP support  See merge request
    !46

* Merge branch 'adadelta' into 'master' [Scott Leishman]

    Adadelta  This is an adaDelta implementation for neon-CPU for
    comparison to @alex cuda-convnet implementation.  There are some rough
    ends that I'd like to get feedback from @scott on: To avoid breaking
    existing yaml files that don't have the &quot;ada&quot; field, MLP.py line 26
    creates this reference if it does not exist. I don't want to change
    all yaml files because this would tempt users to set
    ada['enable']=True for models that don't (yet) support it.  I am using
    a similar hack in layer.py line 167 and there I actually don't know
    what the right way would be to do this, suggestions?  See merge
    request !43

* Merge branch 'distarray' into 'master' [Arjun Bansal]

    minor docs  See merge request !44

* Merge branch 'distarray' into 'master' [Scott Leishman]

    minor additions and cleanup for MPI related features  This should help
    Amir with NERSC/MPI models.  See merge request !42

* Merge branch 'fixpt_backend' into 'master' [Scott Leishman]

    Fixpt backend  Various fixedpoint backend improvements and changes.
    Adds additional tests, smaller network and so forth.  Also slightly
    modified initial logger creation.  See merge request !41

* Add tag to changelog. [Scott Leishman]


## v0.4.0 (2014-10-26)

### Modifications

* v0.4.0 release bump. [Scott Leishman]

* Merge branch 'docs_tests' into 'master' [Scott Leishman]

    Docs tests - MOP cleanup and domain name updates  See merge request
    !40

* Merge branch 'distarray' into 'master' [Scott Leishman]

    ConvnetDist implementation  Distributed convnet implementation and
    MPI/Dist code cleanup  See merge request !39

* Merge branch 'cudaconvnet_integration' into 'master' [Scott Leishman]

    Cudaconvnet integration  See merge request !38

* Merge branch 'master' of gitlab.localdomain:algorithms/neon. [Urs]

    n:algorithms/neon

* style fixes. [Urs]

* added the sparsenet natural image dataset. [Urs Koster]

* Point to cudamat build on new gitlab server. [Scott Leishman]

* Merge branch 'distarray' into 'master' [Scott Leishman]

    Distarray  added distributed backprop for supervised layers  See merge
    request !35

* Changed mylearn to neon in gradient checker. [Anil Thomas]

* Merge branch 'docs_tests' into 'master' [Scott Leishman]

    Branding updates  All instances of mylearn should now be stricken from
    the record (except in the ChangeLog).  Re-ran tests, and one
    experiment, things seem to be ok.  See merge request !34

* Merge branch 'fixpt_backend' into 'master' [Scott Leishman]

    Fixpt backend  Several updates and bug fixes to fixed point backend to
    allow end to end training (only examined MLP's using lots of
    fractional precision so far).  Added new Iris dataset and example
    networks for debugging purposes.  Other minor updates and fixes.  See
    merge request !33

* Merge branch 'distarray' into 'master' [Arjun Bansal]

    Distarray  Added support for dist backprop in fully connected layers
    See merge request !32

* Bugfix for Numpy64 backend. [Scott Leishman]

    Implementation prevented MNIST mlp example from running correctly
    (holdover from previous attempts to pare down Numpy64 backend).

* Merge branch 'distarray' into 'master' [Scott Leishman]

    Distarray  changes up to pretraining of stacked autoencoder  See merge
    request !31

* Added automatic release notes generation.  Closes MYL-15. [Scott Leishman]

    Could stand to be improved.  For instance maybe not including commit
    body, also adding url links for JIRA ticket numbers.


## v0.3.0 (2014-09-25)

### Modifications

* v0.3.0 release bump. [Scott Leishman]

* Added academic use license terms. [Scott Leishman]

    Also removed distributed work in progress from being included in
    release tarball.

* Merge branch 'fixpt_backend' into 'master' [Anil Thomas]

    Fixpt backend  Many changes implementing end-to-end fixed point
    training (configurable on a per tensor basis). * Added Numpy64
    backend, ensure Numpy backend remains 32-bit throughout. * Improved
    numerical stability of cross entropy cost * Easier ability to profile,
    and debug code from application (-p and -d switches)  Currently seeing
    non-convergence in FixedPoint MLP (still investigating, but want to
    get other changes in without further delay) and issues running RBM and
    Google Brain example networks.  See merge request !30

* Merge branch 'google_brain' into 'master' [Scott Leishman]

    Google brain  Fix for LCN backprop.  See merge request !29

* Merge branch 'google_brain' into 'master' [Scott Leishman]

    Google brain  See merge request !28

* Ensure OSX build environment has path to CUDA libs. [Scott Leishman]

* Merge branch 'cuda_compatability' into 'master' [Scott Leishman]

    Cuda compatability.  Fixes MYL-50  Added new CUDA_GPU variable that
    can be used to safely test for presence of CUDA prior to importing
    cuda specific stuff.  Fixed RBM testing, other minor style related
    updates.  See merge request !27

* Merge branch 'gb_minor_fix' into 'master' [Anil Thomas]

    Gb minor updates  * Ensure we can save/generate plots if dirs don't
    yet exist * use platform independent paths * fix minor bug with
    undefined nrecs variable * style conformance, remove some un-needed
    items, other refactoring.  See merge request !26

* Merge branch 'google_brain' into 'master' [Scott Leishman]

    Google brain  Merge from google_brain. Support for sparsity in the
    objective function for unsupervised training. Also integrated CIFAR-10
    dataset.  See merge request !25

* Merge branch 'google_brain' into 'master' [Scott Leishman]

    Google brain  Support for unsupervised pretraining. Also, update from
    master...  See merge request !24

* Cleanup unnecessary factory code. [Scott Leishman]

* Converted Google Brain autoencoder to new YAML object format. [Scott Leishman]

* Merge branch 'experiments_yaml_etc' [Scott Leishman]

    Conflicts:         mylearn/models/dbn.py         mylearn/models/rbm.py
    Resolved conflicts, ensure all tests pass, style is ok, re-ran all
    experiments. * See significant cudamat performance degradation in dot
    product (to investigate). * DBN doesn't fully complete training (to
    investigate). * Google Brain model needs to be updated to conform to
    new YAML python objects.

* Merge branch 'google_brain' into 'master' [Scott Leishman]

    Google brain  Submitting merge request so that the changes to SSE cost
    function gets integrated into master...  See merge request !22

* Merge branch 'google_brain' into 'master' [Anil Thomas]

    Google brain  See merge request !21

* Merge branch 'perf_enhancements' into 'master' [Anil Thomas]

    Perf enhancements  See merge request !20

* Merge branch 'perf_enhancements' into 'master' [Anil Thomas]

    Perf enhancements  See merge request !19

* Merge branch 'rbm_ready_to_master' into 'master' [Scott Leishman]

    Rbm ready to master  merged rbm with master locally and fixed all the
    conflicts, so this should now be ready to go into master. Also made
    sure that Anil's autoencoder still works.  Somebody should check that
    Anil's chances to the cudamat backend have been preserved, as this has
    been automerged and I didn't know what exactly to check to make sure
    it's ok.  See merge request !17

* Merge branch 'fixpt_backend' into 'master' [Scott Leishman]

    Miscellaneous doc and style updates  Added a brief developer guide,
    restored style compliance, added more documentation, and so forth.
    Ignore the branch name (this has nothing to do with fixpt).  Still
    working on MYL-37 but it's a bigger change than anticipated.  See
    merge request !16

* Merge branch 'small_fixes' into 'master' [Scott Leishman]

    Misc. small fixes. (MYL-32, MYL-33)  Correct a few minor issues.  See
    merge request !15

* Merge branch 'google_brain' into 'master' [Anil Thomas]

    Google brain  Hi Scott,  This merge mainly involves support for
    stride. There's also average pooling and LCN, but they are by and
    large independent. I have already updated the branch with changes from
    master, so the merge should be easy.  Thanks, Anil  See merge request
    !14


## v0.2.0 (2014-08-10)

### Modifications

* Version 0.2.0 bump. [Scott Leishman]

* Merge branch 'serialization' [Scott Leishman]

    Conflicts:         mylearn/backends/_numpy.py  Resolved merge
    conflicts, ran all tests and examples.  Things appear to be ok!

* Local filtering and l2 pooling. [Anil Thomas]

*  Merge branch 'cudamat_ops' into 'master' [Scott Leishman]

    in-place operators, missing functionality added  Implemented several
    new in-place arithmetic operators for both backends as well as cleaned
    up some missing implementations (particularly in cudamat backend).
    Found and fixed a couple of issues in layer and model code too.  No
    speed improvements in this release.

*  Merge branch 'cudamat_ops' into 'master' [Scott Leishman]

    Cudamat Ops  Completed removal of get and set related functions that
    broke the numpy interface specification.  Updated all models to use
    numpy standard indexing and slicing syntax.  Implemented small
    benchmark test suite with a handful of benchmarks to examine timing
    performance of indexing and matrix multiplication.

*  Merge branch 'convolutional_neural_networks' into 'master' [Anil Thomas]

    Convolutional Neural Networks  Added code for convolutional and max-
    pooling layers (works only with numpy backend). The cudamat backend
    can now support bias inputs, momentum, relu etc.

*  Merge branch 'audit_existing_code' into 'master' [Scott Leishman]

    Audit Existing Code  Documentation now auto-generated from source code
    comments.  Added pylint checking too.

*  Merge branch 'audit_existing_code' into 'master' [Scott Leishman]

    Audit Existing Code  Integrated flake8 pre-commit hook, audited code
    top-to-bottom to ensure style compliance.

*  Merge branch 'remove_ref' into 'master' [Scott Leishman]

    Remove Ref  remove old reference implementation code which has been
    spun out into a separate project:
    http://192.168.20.2/algorithms/reference_implementations


## v0.1.0 (2014-07-21)

### Modifications

*  Merge branch 'reorg_dev' into 'master' [Scott Leishman]

    create final 0.1.0 release  bump version to signal release.

*  Merge branch 'reorg_dev' into 'master' [Scott Leishman]

    Port backend and layer code from fp_v_fp.  See issue #12  Several
    commits used to transfer code and get simple MLP and autoencoder
    examples running.  Code still needs to be audited, documented, and
    tested.

*  Merge branch 'reorg_dev' into 'master' [Scott Leishman]

    Gitlab continuous integration and test harness setup  Utilizing nose
    for tests, and [tox](http://tox.readthedocs.org/en/latest/) to ensure
    we can test against multiple python versions.  Currently building
    against python2.7 and python3.4

* Merge branch 'master' of 192.168.20.2:algorithms/mylearn. [Anil Thomas]

* Fixed bug in how the column indices of the receptive fields were determined. [Anil Thomas]

*  Merge branch 'reorg_dev' into 'master' [Scott Leishman]

    Python package code layout  Moved files around to follow typical
    python package structure.  Created basic setup.py, other cleanup.

* Caffe configuration files to reproduce the results of CNN6. [Anil Thomas]

* Fixed bug with maxpooling, where the indices corresponding to a pooling region were set incorrectly. [Anil Thomas]

* Sum (instead of averaging) the weight updates on a convolutional layer to match with Caffe. [Anil Thomas]

* Merge branch 'master' of 192.168.20.2:algorithms/mylearn. [Anil Thomas]

* Changes to match with Caffe LeNet implementation. [Anil Thomas]

* Added command line switch to sample/not sample. [Anil Thomas]

* Added momentum to MLP. [Scott Leishman]

    Made some other minor code cleanups.

* Some cleanup. [Anil Thomas]

* Added support for stackable conv layers. [Anil Thomas]

* Maxpooling layer. [Anil Thomas]

* Minibatch training. [Anil Thomas]

* Bugfix. Update the second filter correctly. [Anil Thomas]

* Minibatch training. [Anil Thomas]

* Added softmax and pseudo logistic activations. [Anil Thomas]

* Some cleanup. [Anil Thomas]

* Initialize biases to zero. [Anil Thomas]

* Updated mlp7. [Anil Thomas]

* Added bias inputs. [Anil Thomas]

* Refactored the code. [Anil Thomas]

* Edited comments. [Anil Thomas]

* Enhancements to make convolutions faster. [Anil Thomas]

* Got rid of unnecessary transposes. [Anil Thomas]

* RBM on Theano. [Anil Thomas]

* Moved common code to separate file. [Anil Thomas]

* Refactored the code. [Anil Thomas]

* Replaced logistic with tanh. [Anil Thomas]

* Updated autoencoder. [Anil Thomas]

* Autoencoder. [Anil Thomas]

* CNN version 1. [Anil Thomas]

* Added hyperlink in readme. [Anil Thomas]

* Added hyperlink in readme. [Anil Thomas]

* Updated version 4. [Anil Thomas]

* MLP version 4. [Anil Thomas]

* Updated MLP version 3. [Anil Thomas]

* MLP version 3. [Anil Thomas]

* Updated MLP version 2. [Anil Thomas]

* MLP version 2. The activation and cost functions can be configured now. [Anil Thomas]

* Multilayer Perceptron with numpy - mlp1. [Anil Thomas]

* some modifications to mimic Hinton, Science, 2006 results and Hinton 2012 (dropout) results. [Arjun Bansal]

* got NReLu DBN rate to 3.09% [Arjun Bansal]

* mnist 1.1% [Arjun Bansal]

* updated readme. [Arjun Bansal]

* Updated README with CNN info. [Arjun Bansal]

* MNIST CNN perf around 1.1% [Arjun Bansal]

* for mac want /usr/bin/python. [Arjun Bansal]

* adding tmproj file for MNIST. [Arjun Bansal]

* cleaning up pyc file. [Arjun Bansal]

* MNIST on cuda-convnet. [Arjun Bansal]

* adding cuda-convnet src files. [Arjun Bansal]

* adding directories for CNN and DAE implementations. [Arjun Bansal]

* 1.09% cross entropy reflected in README. [Arjun Bansal]

* Merge branch 'master' of 192.168.20.2:algorithms/mylearn. [Arjun Bansal]

* added cross entropy term to NN to improve MNIST to 1.09% error rate and fixed bugs in numpy/gnumpy version of code in negsampling. [Arjun Bansal]

* added neural network class that can independently achieve 1.39% on MNIST, and can also initialize using pretrained RBM. [Arjun Bansal]

* MNIST DBN code that can achieve around 2.5% error rate. [Arjun Bansal]

* added rbm_cudamat class. [Arjun Bansal]

* added rbm_cudamat class. [Arjun Bansal]

* adding textmate project file. [Arjun Bansal]

* DBN for MNIST, numpy and gnumpy based. [Arjun Bansal]


